# 可爱的贝叶斯

*作者：vophan*   

**为什么说是可爱的贝叶斯呢？那么本文贝叶斯绝对是逃不了关系了...**

我就先从贝叶斯讲起吧。

>贝叶斯(约1702-1761) Thomas Bayes，英国数学家。约1702年出生于伦敦，做过神甫。1742年成为英国皇家学会会员。1761年4月7日逝世。贝叶斯在数学方面主要研究概率论。他首先将归纳推理法用于概率论基础理论，并创立了贝叶斯统计理论，对于统计决策函数、统计推断、统计的估算等做出了贡献。

当然这是百度百科上复制下来的，我想也没有多少人想了解这个老头吧。那么简单的说，他到底做了什么呢？

学过概率论的朋友，一定不会不知道**贝叶斯公式**，贝叶斯公式说简单点就是求条件概率，而说的复杂点就是通过新的事件或者叫证据的加入，来修正先验概率。

公式呢？    

![](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=239894515,405307697&fm=58)

举个例子：(摘自知乎)

>作者：普通熊猫
链接：https://www.zhihu.com/question/51448623/answer/147298455
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
贝叶斯公式与两个概率有关系，一个是先验概率（基础概率），一个是现象概率（观察到的条件）仍然是熊猫给出的例子：某城市发生了一起汽车撞人逃跑事件，该城市只有两种颜色的车，蓝色15%，绿色85%，事发时有一个人在现场看见了，他指证是蓝车。但是根据专家在现场分析,当时那种条件能看正确的可能性是80%。那么,肇事的车是蓝车的概率到底是多少？令B是城市里车为蓝色的事件，G为车子是绿色的事件，E为观察到车子为蓝色的事件。则由已知条件可以得出P(B)=0.15，P(G)=P(~B)=0.85，至于P(E)我们一会儿再说。好了，现在，如果没有证人看到肇事者车的话，那么我们只能盲猜，因此肇事者的车子为蓝色的概率只能是整个城市里面车为蓝色的概率，也就是先验概率P(B)=0.15，因为这时我们还没有其他证据介入，只能做个粗略的估算。接下来，当当当当，有证人了。证人说他看到了车子，并且说是蓝色的，注意，这分两种情况，…………重要的事情说两遍：贝叶斯里面现象(新的证据)部分总是分两种情况出现的：一是车子的确是蓝色的，并且证人也正确的分辨出车是蓝色的来了，概率为 P(E,B)=P(B)xP(E|B)=0.15x0.8=0.12，二是车子根本就是绿色的，只是证人看成蓝色的了，概率为P(E,~B)=P(~B)xP(E|~B)=P(~B)x（1 - P(~E|~B))=0.85x(1-0.8)=0.17，所以P(E)=P(E,B)+P(E,~B)=0.12+0.17=0.29然后，我们要求解的其实是在有证人的条件下车子为蓝色的概率，也就是P(B|E)=P(E,B)/P(E)=0.12/0.29=0.41你看，P(B|E)根本就是P(B)的加强版本，条件概率跟先验概率描述的根本就是同一件事。那么当当当当，又一个结论来了：当有新的证据出现时，P(B|E)会替代原来P(B)的角色。换句话说，现在警察找到了一个新的证人，他也觉得这辆肇事车是蓝色的，这时在新一轮的贝叶斯概率计算中，基础概率P(B)=0.41，而不是原先的0.15，大家可以算一下，新的P(B|E)=0.73，换句话说，当有两个人看见肇事车辆为蓝色的时候，对比只有一个人看到肇事车辆为蓝色的时候，该车实际为蓝色的概率大大增加


## 而我们在机器学习，怎么利用贝叶斯公式呢？

### 朴素贝叶斯分类器:         

***为什么叫朴素呢？请听我慢慢道来。***


对于一件事，可能会有好多的因素来影响他的发生和改变他的性质，而我们要给一个事物分类，也会有好多因素影响我们的判断，我们把这些因素叫做**特征**。
那么，我们怎么给这些事情分类呢？

假设一个事物有2个类别![](http://pic.caigoubao.cc/607328/gs10.png
)，而决定这两个类别的特征有3个![](http://pic.caigoubao.cc/607328/gs9.png
)，而我们需要做的就是求出，这件事物在所有特征的条件下的分别为不同类别的条件概率（![](http://pic.caigoubao.cc/607328/gs1.png
),![](http://pic.caigoubao.cc/607328/gs2.png
)）,如果![](http://pic.caigoubao.cc/607328/gs1.png
)要比![](http://pic.caigoubao.cc/607328/gs2.png
)大，当然这件事就被分类到![](http://pic.caigoubao.cc/607328/gs11.png
)。那么，我们就来算一下：


为了方便计算，我们先假设他只有一个特征X,那么我们来算一下![](http://pic.caigoubao.cc/607328/gs3.png
)

很简单：
![](http://pic.caigoubao.cc/607328/gs4.png
)


我们可以看到：p(c),p(x)是非常容易得到的，而我们唯一要求得的就是![](http://pic.caigoubao.cc/607328/gs12.png
),这也并不难，在大量的训练数据里我们可以找到，特征为x,类别为c的样本数量，除以所有的类别为c的样本数量，就可以得到了。看到这里，你们一定会说：哇，好简单啊，这就是机器学习吗？**可是，现实生活中，没有几件事是可以有一个特征决定其属性的，特征可能会有3个，30个，或者更多。**

那么我们将特征扩展到3个后，再来计算一下，
![](http://pic.caigoubao.cc/607328/gs5.png
),  
同样的，p(c0)和多个因素的联合概率p(x0,x1,x2)是可以单独计算的  

  *注：不知道怎么计算的请翻阅自己的概率论课本*  

  现在我们只需要计算![](http://pic.caigoubao.cc/607328/gs6.png
)了：

根据链式法则：  
我们可以知道![](http://pic.caigoubao.cc/607328/gs7.png
),这才3个特征就这么复杂！！！还让人怎么算。

这时候，**朴素贝叶斯**就出来了，其实我有一点不是很懂的地方，朴素贝叶斯的英文名字叫做：Naive Bayes，中文为什么把naive翻译成朴素呢，我觉得叫做幼稚还比较贴切。
为什么这么说呢？因为：他就像个小孩子，不会做的地方，就糊弄糊弄，就过去了。
那么，他是怎么算的呢？

既然我算不出![](http://pic.caigoubao.cc/607328/gs7.png
)，那么我就假设这些特征都是条件独立的，至于他到底独立不独立，我不知道，我也不管。

所以最终利用朴素贝叶斯，![](http://pic.caigoubao.cc/607328/gs8.png
)

这样就完了。

----


怎么可能？(滑稽  

区区liner regression 还得计算cost函数，最优化cost函数的解呢。之所以简单是因为我们将训练集的特征频率当成了概率，凭什么？

**显然我们不能简单的将频率当成概率**，如果你训练集大的可怕，那还说的过去，如果不大，那就更不行了。

在这里，我们要用到参数估计的方法，来得到决定![](http://pic.caigoubao.cc/607328/gs13.png
)的这些参数（这里假设所有![](http://pic.caigoubao.cc/607328/gs14.png
)符合某种形式，并且他的大小只受到参数θ的影响），而参数估计的方法叫做极大似然估计，考虑文章篇幅，就不再详细介绍极大似然估计了，我们只需要知道，参数估计的方法的步骤：  

1、假设数据集特征的频率符合某种概率分布，比如说：高斯分布  
2、利用训练数据集对概率分布的参数进行估计

以及极大似然估计的原理和步骤:  
原理：所谓极大似然就是让一个参数的似然函数尽可能的取到最大值，而参数θ的似然函数L(θ)就表示分类为c的所有样本在第i个特征上的联合概率分布  
步骤：  
1、确定概率分布函数f(x)  
2、将每个样本的特征值传入f(x)得到一个概率值，然后将所有特征的概率值乘在一起，就得到了似然函数的值。  
3、最大化似然函数的值，求得θ的值  

那么问题又来了，怎么最大化呢？
老方法：**Grident Decent**  

***注意：*** Gredent Decent的操作对象必须是严格的凸函数
在这里，我们就对似然函数的对数进行操作，因为求似然和求他的对数似然效果是一样的，而且可以减少计算量。

最终我们就做好了一个，朴素贝叶斯的分类器，其实说句实话，这些东西都有人做好了，sklearn的库中写的非常完美，但是我们还是得知道原理。
![](http://pic.caigoubao.cc/607328/note1.PNG
)
